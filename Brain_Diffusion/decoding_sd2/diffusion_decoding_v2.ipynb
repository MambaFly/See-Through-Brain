{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Python路径: ['/data/zlhu/NeuroAI/Eye-of-Brain/Brain-Diffusion/decoding_sd2/stablediffusion', '/data/zlhu/NeuroAI/Eye-of-Brain/Brain-Diffusion/decoding_sd2/stablediffusion', '/data/zlhu', '/data/zlhu/miniconda3/envs/brain/lib/python38.zip', '/data/zlhu/miniconda3/envs/brain/lib/python3.8', '/data/zlhu/miniconda3/envs/brain/lib/python3.8/lib-dynload', '', '/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages', '/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/bdpy-0.18-py3.8.egg']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 清除任何可能导致冲突的路径\n",
    "for i in range(len(sys.path)-1, -1, -1):\n",
    "    if \"StableDiffusionReconstruction\" in sys.path[i] or \"diffusion_sd1\" in sys.path[i]:\n",
    "        sys.path.pop(i)\n",
    "\n",
    "# 添加正确的路径\n",
    "sys.path.insert(0, '/data/zlhu/NeuroAI/Eye-of-Brain/Brain-Diffusion/decoding_sd2/stablediffusion')\n",
    "print(\"当前Python路径:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已存在\n"
     ]
    }
   ],
   "source": [
    "import argparse, os\n",
    "from tqdm import tqdm, trange\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from einops import repeat, rearrange\n",
    "from pytorch_lightning import seed_everything\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.data.util import AddMiDaS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(config, ckpt,device):\n",
    "    config = OmegaConf.load(config)\n",
    "    model = instantiate_from_config(config.model)\n",
    "    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n",
    "    model = model.to(device)\n",
    "    sampler = DDIMSampler(model)\n",
    "    return sampler\n",
    "    \n",
    "def initialize_model(config, ckpt, device):\n",
    "    config = OmegaConf.load(config)\n",
    "    print(f\"模型目标: {config.model.target}\")\n",
    "    \n",
    "    # 直接导入正确路径的模块\n",
    "    from stablediffusion.ldm.util import instantiate_from_config\n",
    "    model = instantiate_from_config(config.model)\n",
    "    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n",
    "    model = model.to(device)\n",
    "    from stablediffusion.ldm.models.diffusion.ddim import DDIMSampler\n",
    "    sampler = DDIMSampler(model)\n",
    "    return sampler\n",
    "def pad_image(input_image):\n",
    "    pad_w, pad_h = np.max(((2, 2), np.ceil(\n",
    "        np.array(input_image.size) / 64).astype(int)), axis=0) * 64 - input_image.size\n",
    "    im_padded = Image.fromarray(\n",
    "        np.pad(np.array(input_image), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n",
    "    return im_padded\n",
    "\n",
    "\n",
    "def make_batch_sd(\n",
    "        image,\n",
    "        txt,\n",
    "        device,\n",
    "        num_samples=1,\n",
    "        model_type=\"dpt_hybrid\"\n",
    "):\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n",
    "    # sample['jpg'] is tensor hwc in [-1, 1] at this point\n",
    "    midas_trafo = AddMiDaS(model_type=model_type)\n",
    "    batch = {\n",
    "        \"jpg\": image,\n",
    "        \"txt\": txt,\n",
    "    }\n",
    "    batch = midas_trafo(batch)\n",
    "    batch[\"jpg\"] = rearrange(batch[\"jpg\"], 'h w c -> 1 c h w')\n",
    "    batch[\"jpg\"] = repeat(batch[\"jpg\"].to(device=device),\n",
    "                          \"1 ... -> n ...\", n=num_samples)\n",
    "    batch[\"midas_in\"] = repeat(torch.from_numpy(batch[\"midas_in\"][None, ...]).to(\n",
    "        device=device), \"1 ... -> n ...\", n=num_samples)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "imgidxs = [0, 10]\n",
    "gpu = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "subject='subj01'\n",
    "config = './stablediffusion/configs/stable-diffusion/v2-midas-inference.yaml'\n",
    "ckpt = './stablediffusion/models/512-depth-ema.ckpt'\n",
    "steps = 50\n",
    "scale = 5.0\n",
    "eta = 0.0\n",
    "strength = 0.8\n",
    "num_samples= 1\n",
    "callback=None\n",
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 'autocast'\n",
    "precision_scope = autocast if precision == \"autocast\" else nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: /data\n",
      "当前工作目录: /data/zlhu/NeuroAI/Eye-of-Brain/Brain-Diffusion/decoding_sd2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 获取当前文件所在目录\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "print(\"当前工作目录:\", current_dir)\n",
    "# 设置工作目录\n",
    "os.chdir('/data/zlhu/NeuroAI/Eye-of-Brain/Brain-Diffusion/decoding_sd2')\n",
    "# 验证工作目录\n",
    "print(\"当前工作目录:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型目标: ldm.models.diffusion.ddpm.LatentDepth2ImageDiffusion\n",
      "文件已存在\n",
      "No module 'xformers'. Proceeding without it.\n",
      "LatentDepth2ImageDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/timm/models/layers/__init__.py:49: DeprecationWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", DeprecationWarning)\n",
      "/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/open_clip/factory.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
      "/data/zlhu/miniconda3/envs/brain/lib/python3.8/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "/data/zlhu/NeuroAI/Eye-of-Brain/Brain-Diffusion/decoding_sd2/stablediffusion/ldm/modules/midas/midas/base_model.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  parameters = torch.load(path, map_location=torch.device('cpu'))\n",
      "/tmp/ipykernel_38175/2277422062.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f\"cuda:{gpu}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "sampler = initialize_model(config,ckpt,device)\n",
    "model = sampler.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected timesteps for ddim sampler: [  1  21  41  61  81 101 121 141 161 181 201 221 241 261 281 301 321 341\n",
      " 361 381 401 421 441 461 481 501 521 541 561 581 601 621 641 661 681 701\n",
      " 721 741 761 781 801 821 841 861 881 901 921 941 961 981]\n",
      "Selected alphas for ddim sampler: a_t: tensor([0.9983, 0.9804, 0.9609, 0.9398, 0.9171, 0.8930, 0.8674, 0.8404, 0.8121,\n",
      "        0.7827, 0.7521, 0.7207, 0.6885, 0.6557, 0.6224, 0.5888, 0.5551, 0.5215,\n",
      "        0.4882, 0.4552, 0.4229, 0.3913, 0.3605, 0.3308, 0.3023, 0.2750, 0.2490,\n",
      "        0.2245, 0.2014, 0.1799, 0.1598, 0.1413, 0.1243, 0.1087, 0.0946, 0.0819,\n",
      "        0.0705, 0.0604, 0.0514, 0.0435, 0.0365, 0.0305, 0.0254, 0.0210, 0.0172,\n",
      "        0.0140, 0.0113, 0.0091, 0.0073, 0.0058]); a_(t-1): [0.99914998 0.99829602 0.98038077 0.96087277 0.93978298 0.91713792\n",
      " 0.89298052 0.86737001 0.84038192 0.81210774 0.78265446 0.75214338\n",
      " 0.72070938 0.68849909 0.65566933 0.62238538 0.58881873 0.55514455\n",
      " 0.52153981 0.4881804  0.45523876 0.42288151 0.39126703 0.36054322\n",
      " 0.33084565 0.30229566 0.27499905 0.24904492 0.22450483 0.20143245\n",
      " 0.1798636  0.15981644 0.14129217 0.12427604 0.10873855 0.09463691\n",
      " 0.08191671 0.0705137  0.06035557 0.05136392 0.043456   0.03654652\n",
      " 0.03054927 0.02537862 0.02095082 0.01718517 0.0140049  0.01133791\n",
      " 0.00911731 0.00728173]\n",
      "For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], dtype=torch.float64)\n",
      "target t_enc is 40 steps\n"
     ]
    }
   ],
   "source": [
    "'''采样策略'''\n",
    "sampler.make_schedule(ddim_num_steps=steps, ddim_eta=eta, verbose=True)\n",
    "\n",
    "assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "\n",
    "t_enc = min(int(strength * steps), steps-1)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Prediction (C, InitLatent, Depth(cc))\n",
    "captdir = f'../../Brain-Caption/decoded_captions/{subject}'\n",
    "dptdir = f'../../data/decoded/{subject}/dpt_fromemb/'\n",
    "# gandir = f'../../decoded/gan_recon_img/all_layers/{subject}/streams/'\n",
    "\n",
    "# C\n",
    "captions = pd.read_csv(f'{captdir}/captions_brain.csv', sep='\\t',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy.io\n",
    "from nsd_access.nsda import NSDAccess\n",
    "# Load NSD information\n",
    "\n",
    "nsd_expdesign = scipy.io.loadmat('../../data/nsd/nsddata/experiments/nsd/nsd_expdesign.mat')\n",
    "\n",
    "nsda = NSDAccess('../../data/nsd/')\n",
    "sf = h5py.File(nsda.stimuli_file, 'r')\n",
    "sdataset = sf.get('imgBrick')\n",
    "\n",
    "stims_ave = np.load(f'../../data/stim/{subject}/{subject}_stims_ave.npy')\n",
    "\n",
    "# Note that mos of them are 1-base index!\n",
    "# This is why I subtract 1\n",
    "sharedix = nsd_expdesign['sharedix'] -1 \n",
    "tr_idx = np.zeros_like(stims_ave)\n",
    "for idx, s in enumerate(stims_ave):\n",
    "    if s in sharedix:\n",
    "        tr_idx[idx] = 0\n",
    "    else:\n",
    "        tr_idx[idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now is img0.../n\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tr_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow is img\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgidx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.../n\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load z (Image)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m imgidx_te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mtr_idx\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m][imgidx] \u001b[38;5;66;03m# Extract test image index\u001b[39;00m\n\u001b[1;32m     15\u001b[0m idx73k\u001b[38;5;241m=\u001b[39m stims_ave[imgidx_te]\n\u001b[1;32m     16\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39msqueeze(sdataset[idx73k,:,:,:])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     17\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sample_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgidx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_org.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tr_idx' is not defined"
     ]
    }
   ],
   "source": [
    "### decoding NO.1 第一版\n",
    "\n",
    "# Save Directories\n",
    "outdir = f'../../data/decoded/{subject}/image-depth-v1/'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "sample_path = os.path.join(outdir, f\"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "\n",
    "# padding-batch\n",
    "for imgidx in range(10):\n",
    "    \n",
    "    print(f\"now is img{imgidx}.../n\")\n",
    "    # Load z (Image)\n",
    "    imgidx_te = np.where(tr_idx==0)[0][imgidx] # Extract test image index\n",
    "    idx73k= stims_ave[imgidx_te]\n",
    "    Image.fromarray(np.squeeze(sdataset[idx73k,:,:,:]).astype(np.uint8)).save(\n",
    "        os.path.join(sample_path, f\"{imgidx:05}_org.png\"))\n",
    "    \n",
    "    # get caption and depth\n",
    "    prompt = [captions.iloc[imgidx][0]]\n",
    "    cc = torch.Tensor(np.load(f'{dptdir}/{imgidx:06}.npy')).to('cuda')\n",
    "    \n",
    "    # # Generate image from Text + GAN + Depth\n",
    "    # shenpath = f'{gandir}/recon_image_normalized-VGG19-fc8-{subject}-streams-{imgidx:06}.tiff'\n",
    "    # init_image = Image.open(shenpath).resize((512,512))\n",
    "    \n",
    "    # Generate image from Text + Depth\n",
    "    \n",
    "    roi_latent = 'early'\n",
    "    scores_latent = np.load(f'../data/decoded/{subject}/{subject}_{roi_latent}_brain_embs_init_latent.npy')\n",
    "    imgarr = torch.Tensor(scores_latent[imgidx,:].reshape(4,40,40)).unsqueeze(0).to('cuda')\n",
    "    \n",
    "    # Generate image from Z\n",
    "    precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                x_samples = model.decode_first_stage(imgarr)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    \n",
    "                for x_sample in x_samples:\n",
    "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "    init_image = Image.fromarray(x_sample.astype(np.uint8)).resize((512,512))\n",
    "    image = pad_image(init_image)\n",
    "    \n",
    "    base_count = 0\n",
    "    with torch.no_grad():\n",
    "        for n in trange(n_iter, desc=\"Sampling\"):\n",
    "            torch.autocast(\"cuda\")\n",
    "            batch = make_batch_sd(image, txt=prompt, device=device, num_samples=num_samples)\n",
    "            z = model.get_first_stage_encoding(model.encode_first_stage(\n",
    "                batch[model.first_stage_key]))  # move to latent space\n",
    "            c = model.cond_stage_model.encode(batch[\"txt\"]).mean(axis=0).unsqueeze(0)\n",
    "            c_cat = list()\n",
    "            c_cat.append(cc)\n",
    "            c_cat = torch.cat(c_cat, dim=1)\n",
    "    \n",
    "            # cond\n",
    "            cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n",
    "    \n",
    "            # uncond cond\n",
    "            uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n",
    "            uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n",
    "    \n",
    "            # encode (scaled latent)\n",
    "            z_enc = sampler.stochastic_encode(\n",
    "                z, torch.tensor([t_enc] * num_samples).to(model.device))\n",
    "    \n",
    "            # decode it\n",
    "            samples = sampler.decode(z_enc, cond, t_enc, unconditional_guidance_scale=scale,\n",
    "                                    unconditional_conditioning=uc_full, callback=callback)\n",
    "            x_samples_ddim = model.decode_first_stage(samples)\n",
    "            result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "            result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n",
    "            Image.fromarray(result[0,:,:,:].astype(np.uint8)).save(\n",
    "                os.path.join(sample_path, f\"{imgidx:05}_{base_count:03}.png\"))   \n",
    "            base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.22it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.20it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.19it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:25, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.18it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:12, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.18it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.00s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.17it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.17it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.17it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.07s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.10s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.11s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.14s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.14it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.14it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.14it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.16s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.14it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.15s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.14it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.14s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.13s/it]\n",
      "Sampling:   0%|                                                         | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  20%|█████████▊                                       | 1/5 [00:13<00:52, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.16it/s]\n",
      "Sampling:  40%|███████████████████▌                             | 2/5 [00:26<00:39, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  60%|█████████████████████████████▍                   | 3/5 [00:39<00:26, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling:  80%|███████████████████████████████████████▏         | 4/5 [00:52<00:13, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DDIM Sampling with 40 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|█████████████████████████████████████████| 40/40 [00:12<00:00,  3.15it/s]\n",
      "Sampling: 100%|█████████████████████████████████████████████████| 5/5 [01:05<00:00, 13.13s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "### decoding NO.2 第二版\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def load_img_from_arr(img_arr):\n",
    "    image = Image.fromarray(img_arr).convert(\"RGB\")\n",
    "    w, h = 512, 512\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "# Save Directories\n",
    "outdir = f'../../data/decoded/{subject}/image-depth-v2/'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "sample_path = os.path.join(outdir, f\"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "\n",
    "for imgidx in range(10):\n",
    "\n",
    "    c = model.get_learned_conditioning(captions.iloc[imgidx][0]).to('cuda')\n",
    "    cc = torch.Tensor(np.load(f'{dptdir}/{imgidx:06}.npy')).to('cuda')\n",
    "    \n",
    "    # # Generate image from Text + GAN + Depth\n",
    "    # shenpath = f'{gandir}/recon_image_normalized-VGG19-fc8-{subject}-streams-{imgidx:06}.tiff'\n",
    "    # init_image = Image.open(shenpath).resize((512,512))\n",
    "    \n",
    "    # Generate image from Text + Depth\n",
    "    \n",
    "    roi_latent = 'early'\n",
    "    scores_latent = np.load(f'../data/decoded/{subject}/{subject}_{roi_latent}_brain_embs_init_latent.npy')\n",
    "    imgarr = torch.Tensor(scores_latent[imgidx,:].reshape(4,40,40)).unsqueeze(0).to('cuda')\n",
    "    \n",
    "    # Generate image from Z\n",
    "    precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                x_samples = model.decode_first_stage(imgarr)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "    \n",
    "                for x_sample in x_samples:\n",
    "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "    \n",
    "    im = Image.fromarray(x_sample.astype(np.uint8)).resize((512,512))\n",
    "    im = np.array(im)\n",
    "    init_image = load_img_from_arr(im).to('cuda')\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "    \n",
    "    base_count = 0\n",
    "    with torch.no_grad():\n",
    "        for n in trange(n_iter, desc=\"Sampling\"):\n",
    "            with torch.autocast(\"cuda\"):\n",
    "                c_cat = list()\n",
    "                c_cat.append(cc)\n",
    "                c_cat = torch.cat(c_cat, dim=1)\n",
    "        \n",
    "                # cond\n",
    "                cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n",
    "        \n",
    "                # uncond cond\n",
    "                uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n",
    "                uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n",
    "        \n",
    "                # encode (scaled latent)\n",
    "                z_enc = sampler.stochastic_encode(\n",
    "                    init_latent, torch.tensor([t_enc] * num_samples).to(model.device))\n",
    "        \n",
    "                # decode it\n",
    "                samples = sampler.decode(z_enc, cond, t_enc, unconditional_guidance_scale=scale,\n",
    "                                        unconditional_conditioning=uc_full, callback=callback)\n",
    "                x_samples_ddim = model.decode_first_stage(samples)\n",
    "                result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n",
    "                Image.fromarray(result[0,:,:,:].astype(np.uint8)).save(\n",
    "                    os.path.join(sample_path, f\"{imgidx:05}_{base_count:03}.png\"))   \n",
    "                base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
